# LLM Backend ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•åœ¨æ–‡å­—å†’é™©æ¸¸æˆé¡¹ç›®ä¸­ä½¿ç”¨ LLM åç«¯ç³»ç»Ÿã€‚

---

## ğŸ“š ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ¶æ„æ¦‚è§ˆ](#æ¶æ„æ¦‚è§ˆ)
3. [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
4. [æ¸¸æˆå·¥å…·](#æ¸¸æˆå·¥å…·)
5. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
6. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## å¿«é€Ÿå¼€å§‹

### æ–¹æ³• 1: ä½¿ç”¨é…ç½®åŠ è½½å™¨ (æ¨è)

```python
from llm.agent_config import load_agent_backend
from llm.base import LLMMessage

# åŠ è½½é¢„é…ç½®çš„ Agent
game_master = load_agent_backend("game_master")

# ç”Ÿæˆå“åº”
messages = [
    LLMMessage(role="system", content="ä½ æ˜¯æ¸¸æˆä¸»æŒäºº"),
    LLMMessage(role="user", content="å¼€å§‹æ¸¸æˆ")
]

response = await game_master.generate(messages)
print(response.content)
```

### æ–¹æ³• 2: ç›´æ¥åˆ›å»ºåç«¯

```python
from llm import create_backend
from llm.base import LLMMessage

# åˆ›å»º LiteLLM åç«¯ (æˆæœ¬ä½)
litellm_backend = create_backend("litellm", {
    "model": "deepseek",
    "temperature": 0.7
})

# åˆ›å»º Claude åç«¯ (åŠŸèƒ½å¼º)
claude_backend = create_backend("claude", {
    "use_litellm_proxy": True,  # é€šè¿‡ LiteLLM ä»£ç†
    "model": "deepseek",
    "temperature": 0.8,
    "allowed_tools": ["Read", "Write"]
})

# ä½¿ç”¨
messages = [LLMMessage(role="user", content="ä½ å¥½")]
response = await claude_backend.generate(messages)
```

---

## æ¶æ„æ¦‚è§ˆ

### ä¸‰ç§ä½¿ç”¨æ¨¡å¼

```
æ¨¡å¼ 1: LiteLLM ç›´æ¥è°ƒç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   åº”ç”¨   â”‚ --> â”‚ LiteLLM  â”‚ --> â”‚ DeepSeek â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æˆæœ¬: ä½ | åŠŸèƒ½: åŸºç¡€ | é€‚ç”¨: ç®€å•å¯¹è¯

æ¨¡å¼ 2: Claude Agent SDK ç›´æ¥è°ƒç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   åº”ç”¨   â”‚ --> â”‚ Claude   â”‚ --> â”‚  Claude  â”‚
â”‚          â”‚     â”‚Agent SDK â”‚     â”‚   API    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æˆæœ¬: é«˜ | åŠŸèƒ½: å®Œæ•´ | é€‚ç”¨: é«˜è´¨é‡ä»»åŠ¡

æ¨¡å¼ 3: Claude Agent SDK + LiteLLM ä»£ç† (æ¨è!)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   åº”ç”¨   â”‚ --> â”‚ Claude   â”‚ --> â”‚ LiteLLM  â”‚ --> â”‚ DeepSeek â”‚
â”‚          â”‚     â”‚Agent SDK â”‚     â”‚  Proxy   â”‚     â”‚ /Qwen/... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æˆæœ¬: ä½ | åŠŸèƒ½: å®Œæ•´ | é€‚ç”¨: æ‰€æœ‰åœºæ™¯
```

### ç»Ÿä¸€æ¥å£

æ‰€æœ‰åç«¯éƒ½å®ç°äº† `LLMBackend` æ¥å£:

```python
class LLMBackend(ABC):
    async def generate(messages, tools=None, **kwargs) -> LLMResponse
    async def generate_structured(messages, response_schema, **kwargs) -> Dict
    async def generate_stream(messages, **kwargs) -> AsyncIterator[str]
```

---

## ä½¿ç”¨æ–¹æ³•

### 1. åŸºç¡€æ–‡æœ¬ç”Ÿæˆ

```python
from llm.agent_config import load_agent_backend
from llm.base import LLMMessage

# åŠ è½½ Agent
gm = load_agent_backend("game_master")

# ç”Ÿæˆæ–‡æœ¬
messages = [
    LLMMessage(role="system", content="ä½ æ˜¯æ¸¸æˆä¸»æŒäºº"),
    LLMMessage(role="user", content="æè¿°ä¸€ä¸ªç¥ç§˜çš„æ£®æ—")
]

response = await gm.generate(messages)
print(response.content)  # ç”Ÿæˆçš„æ–‡æœ¬
print(response.model)    # ä½¿ç”¨çš„æ¨¡å‹
print(response.metadata) # é¢å¤–ä¿¡æ¯ (tokens, latencyç­‰)
```

### 2. ç»“æ„åŒ–è¾“å‡º

```python
# å®šä¹‰å“åº” Schema
schema = {
    "type": "object",
    "properties": {
        "location": {"type": "string"},
        "description": {"type": "string"},
        "mood": {"type": "string", "enum": ["mysterious", "dangerous", "peaceful"]}
    },
    "required": ["location", "description", "mood"]
}

# ç”Ÿæˆç»“æ„åŒ–å“åº”
result = await gm.generate_structured(
    messages=[LLMMessage(role="user", content="åˆ›å»ºä¸€ä¸ªæ–°åœ°ç‚¹")],
    response_schema=schema
)

print(result["location"])     # "è¿·é›¾æ£®æ—"
print(result["description"])  # "ç¬¼ç½©åœ¨æ°¸æ’è¿·é›¾ä¸­çš„å¤è€æ£®æ—..."
print(result["mood"])         # "mysterious"
```

### 3. æµå¼è¾“å‡º

```python
# æµå¼ç”Ÿæˆ (ç”¨äºå®æ—¶æ˜¾ç¤º)
async for chunk in gm.generate_stream(messages):
    print(chunk, end="", flush=True)
```

### 4. å·¥å…·è°ƒç”¨ (ä»… Claude Backend)

```python
from llm import create_backend
from llm.base import LLMTool, LLMMessage

# å®šä¹‰å·¥å…·
tools = [
    LLMTool(
        name="check_inventory",
        description="æŸ¥çœ‹ç©å®¶èƒŒåŒ…",
        input_schema={
            "type": "object",
            "properties": {}
        }
    )
]

# åˆ›å»ºæ”¯æŒå·¥å…·çš„ Claude Backend
claude = create_backend("claude", {
    "use_litellm_proxy": True,
    "model": "deepseek",
    "allowed_tools": ["check_inventory"]
})

# ç”Ÿæˆå“åº” (Agent ä¼šè‡ªåŠ¨è°ƒç”¨å·¥å…·)
response = await claude.generate(
    messages=[LLMMessage(role="user", content="æˆ‘çš„èƒŒåŒ…é‡Œæœ‰ä»€ä¹ˆ?")],
    tools=tools
)

# æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†å·¥å…·
if response.tool_calls:
    for tool_call in response.tool_calls:
        print(f"è°ƒç”¨å·¥å…·: {tool_call.name}")
        print(f"å‚æ•°: {tool_call.arguments}")
```

---

## æ¸¸æˆå·¥å…·

### å¯ç”¨å·¥å…·åˆ—è¡¨

æ¸¸æˆä¸“ç”¨çš„ MCP Server æä¾›äº†ä»¥ä¸‹å·¥å…·:

#### éª°å­å’Œæ£€å®š
- `roll_dice`: æŠ•éª°å­
- `skill_check`: æŠ€èƒ½æ£€å®š

#### ç©å®¶çŠ¶æ€
- `check_status`: æŸ¥çœ‹è§’è‰²çŠ¶æ€
- `update_player_hp`: æ›´æ–°ç”Ÿå‘½å€¼
- `update_player_stamina`: æ›´æ–°ä½“åŠ›å€¼

#### ç‰©å“ç®¡ç†
- `check_inventory`: æŸ¥çœ‹èƒŒåŒ…
- `use_item`: ä½¿ç”¨ç‰©å“
- `add_item`: æ·»åŠ ç‰©å“
- `remove_item`: ç§»é™¤ç‰©å“

#### åœ°å›¾æ¢ç´¢
- `check_map`: æŸ¥çœ‹åœ°å›¾
- `check_surroundings`: ç¯é¡¾å››å‘¨
- `unlock_location`: è§£é”æ–°åœ°ç‚¹
- `set_location`: è®¾ç½®ç©å®¶ä½ç½®

#### ä»»åŠ¡ç³»ç»Ÿ
- `check_quests`: æŸ¥çœ‹ä»»åŠ¡åˆ—è¡¨

#### NPC äº¤äº’
- `talk_to_npc`: ä¸ NPC å¯¹è¯
- `trade_with_npc`: ä¸ NPC äº¤æ˜“

#### æ ‡è®°å’Œå¥–åŠ±
- `set_flag`: è®¾ç½®æ¸¸æˆæ ‡è®°
- `award_experience`: å¥–åŠ±ç»éªŒå€¼

### ä½¿ç”¨æ¸¸æˆå·¥å…·

```python
from claude_agent_sdk import query, ClaudeAgentOptions
from llm.game_tools_mcp import create_game_tools_server, get_game_tool_names
import anyio

async def use_game_tools():
    # åˆ›å»ºæ¸¸æˆå·¥å…· MCP Server
    game_tools = create_game_tools_server()

    # è·å–æ‰€æœ‰å·¥å…·åç§°
    tool_names = get_game_tool_names()

    # é…ç½® Agent
    opts = ClaudeAgentOptions(
        mcp_servers={"game-tools": game_tools},
        allowed_tools=tool_names,
        max_turns=5
    )

    # ä½¿ç”¨å·¥å…·
    async for msg in query(
        prompt="å¸®æˆ‘æŠ•ä¸€ä¸ª20é¢éª°å­,ç„¶åæŸ¥çœ‹æˆ‘çš„èƒŒåŒ…",
        options=opts
    ):
        print(msg)

anyio.run(use_game_tools)
```

### å·¥å…·è¿”å›æ ¼å¼

æ‰€æœ‰æ¸¸æˆå·¥å…·éƒ½è¿”å›ç»Ÿä¸€æ ¼å¼:

```python
{
    "content": [
        {
            "type": "text",
            "text": "ğŸ“¦ æŸ¥çœ‹èƒŒåŒ…"
        }
    ],
    "metadata": {
        "tool_name": "check_inventory",
        "action": "query_inventory"
        # ... å…¶ä»–å…ƒæ•°æ®
    }
}
```

æ¸¸æˆå¼•æ“å¯ä»¥é€šè¿‡ `metadata` ä¸­çš„ `action` å­—æ®µæ¥æ‰§è¡Œå®é™…æ“ä½œã€‚

---

## é…ç½®è¯´æ˜

### å…¨å±€é…ç½® (config/llm_backend.yaml)

```yaml
# é€‰æ‹©é»˜è®¤åç«¯
backend: "litellm"  # æˆ– "claude"

# LiteLLM é…ç½®
litellm:
  config_path: "./config/litellm_config.yaml"
  model: "deepseek"
  temperature: 0.7
  max_tokens: 1000

# Claude é…ç½®
claude:
  use_litellm_proxy: true  # æ˜¯å¦ä½¿ç”¨ LiteLLM ä»£ç†
  model: "deepseek"
  temperature: 0.7
  max_tokens: 4096
  allowed_tools: ["Read", "Write", "Bash"]
```

### Agent é…ç½® (config/llm_agents.yaml)

```yaml
global:
  litellm_proxy_url: "http://0.0.0.0:4000"
  litellm_master_key: ${LITELLM_MASTER_KEY}

agents:
  game_master:
    backend: "claude"
    use_litellm_proxy: true
    model: "deepseek"  # æˆæœ¬ä½,ä¸­æ–‡å¥½
    temperature: 0.8
    max_tokens: 2000
    allowed_tools: ["Read", "Write", "Bash"]

  npc_dialogue:
    backend: "claude"
    use_litellm_proxy: true
    model: "qwen"  # ä¸­æ–‡å¯¹è¯ä¼˜åŒ–
    temperature: 0.9
    max_tokens: 1000

  world_generator:
    backend: "claude"
    use_litellm_proxy: true
    model: "claude-sonnet"  # é«˜è´¨é‡åˆ›ä½œ
    temperature: 0.8
    max_tokens: 3000
```

### LiteLLM æ¨¡å‹é…ç½® (config/litellm_config.yaml)

```yaml
model_list:
  - model_name: deepseek
    litellm_params:
      model: openrouter/deepseek/deepseek-v3.1-terminus
      api_key: ${OPENROUTER_API_KEY}

  - model_name: qwen
    litellm_params:
      model: openrouter/qwen/qwen-2.5-72b-instruct
      api_key: ${OPENROUTER_API_KEY}

  - model_name: claude-sonnet
    litellm_params:
      model: openrouter/anthropic/claude-3.5-sonnet
      api_key: ${OPENROUTER_API_KEY}
```

---

## æœ€ä½³å®è·µ

### 1. æ ¹æ®ä»»åŠ¡é€‰æ‹©åç«¯

```python
# ç®€å•å¯¹è¯ â†’ LiteLLM + DeepSeek (ä¾¿å®œ)
simple_agent = create_backend("litellm", {"model": "deepseek"})

# éœ€è¦å·¥å…·è°ƒç”¨ â†’ Claude + LiteLLM Proxy (Agentèƒ½åŠ›)
tool_agent = create_backend("claude", {
    "use_litellm_proxy": True,
    "model": "deepseek",
    "allowed_tools": ["check_inventory", "roll_dice"]
})

# é«˜è´¨é‡åˆ›ä½œ â†’ Claude + Claude API (è´¨é‡é«˜)
creative_agent = create_backend("claude", {
    "use_litellm_proxy": False,
    "model": "claude-sonnet-4-20250514"
})
```

### 2. ä½¿ç”¨ Agent é…ç½®ç³»ç»Ÿ

```python
# æ¨è: ä½¿ç”¨é…ç½®æ–‡ä»¶ç®¡ç†å¤šä¸ª Agent
from llm.agent_config import load_agent_backend

gm = load_agent_backend("game_master")
npc = load_agent_backend("npc_dialogue")
world = load_agent_backend("world_generator")

# ä¼˜ç‚¹:
# - é›†ä¸­ç®¡ç†é…ç½®
# - æ˜“äºåˆ‡æ¢æ¨¡å‹
# - æˆæœ¬å¯æ§
```

### 3. åˆç†è®¾ç½®æ¸©åº¦

```python
# é€»è¾‘è®¡ç®—ã€æˆ˜æ–—ç³»ç»Ÿ â†’ ä½æ¸©åº¦ (0.3-0.5)
combat = create_backend("litellm", {
    "model": "deepseek",
    "temperature": 0.5
})

# æ­£å¸¸å¯¹è¯ â†’ ä¸­æ¸©åº¦ (0.6-0.8)
dialogue = create_backend("litellm", {
    "model": "qwen",
    "temperature": 0.7
})

# åˆ›æ„å†…å®¹ã€å™äº‹ç”Ÿæˆ â†’ é«˜æ¸©åº¦ (0.8-1.0)
creative = create_backend("claude", {
    "model": "claude-sonnet",
    "temperature": 0.9
})
```

### 4. é”™è¯¯å¤„ç†

```python
from llm.base import LLMError

try:
    response = await gm.generate(messages)
except LLMError as e:
    print(f"LLM é”™è¯¯: {e}")
    # é™çº§å¤„ç†æˆ–é‡è¯•
```

### 5. ç›‘æ§æˆæœ¬

```python
# è®°å½•æ¯æ¬¡è°ƒç”¨
async def generate_with_logging(backend, messages):
    response = await backend.generate(messages)

    # ä» metadata è·å– token ä½¿ç”¨æƒ…å†µ
    tokens = response.metadata.get("tokens", {})
    model = response.model

    # è®¡ç®—æˆæœ¬
    cost_per_1k = {
        "deepseek": 0.0007,
        "qwen": 0.0014,
        "claude-sonnet": 0.011
    }

    total_tokens = tokens.get("total", 0)
    cost = (total_tokens / 1000) * cost_per_1k.get(model, 0)

    print(f"[COST] {model}: ${cost:.4f} ({total_tokens} tokens)")

    return response
```

---

## ç›¸å…³æ–‡æ¡£

- [LiteLLM + Claude Agent SDK å®Œæ•´æŒ‡å—](./LITELLM_AGENT_GUIDE.md)
- [OpenRouter é…ç½®æŒ‡å—](./guides/OPENROUTER_SETUP.md)
- [é¡¹ç›®å¿«é€Ÿå‚è€ƒ](./QUICK_REFERENCE.md)

---

**æœ€åæ›´æ–°**: 2025-11-01
**ç‰ˆæœ¬**: v1.0
