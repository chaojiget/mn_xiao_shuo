# æµå¼èŠå¤©åŠŸèƒ½ - å¿«é€Ÿæ€»ç»“

## âœ¨ å·²å®ç°åŠŸèƒ½

### 1. èŠå¤©é¡µé¢
- **è·¯å¾„**: http://localhost:3000/chat
- **è®¾è®¡**: ChatGPT é£æ ¼,æ¯æ¡æ¶ˆæ¯ä¸€ä¸ªæ°”æ³¡
- **æµå¼è¾“å‡º**: å®æ—¶æ˜¾ç¤º AI ç”Ÿæˆçš„æ–‡æœ¬
- **ç‰¹æ€§**:
  - ç”¨æˆ·æ¶ˆæ¯:è“è‰²æ¸å˜,å³å¯¹é½
  - AI æ¶ˆæ¯:åŠé€æ˜ç™½è‰²,å·¦å¯¹é½
  - æµå¼è¾“å‡ºæ—¶æ˜¾ç¤ºé—ªçƒå…‰æ ‡
  - Enter å‘é€,Shift+Enter æ¢è¡Œ
  - è‡ªåŠ¨æ»šåŠ¨åˆ°æœ€æ–°æ¶ˆæ¯

### 2. åç«¯ API
- **ç«¯ç‚¹**: `POST /api/chat/stream`
- **æ ¼å¼**: Server-Sent Events (SSE)
- **å½“å‰çŠ¶æ€**: æ¨¡æ‹Ÿæµå¼å“åº”(æ¼”ç¤ºç”¨)

## ğŸš€ ä½¿ç”¨æ–¹æ³•

1. è®¿é—® http://localhost:3000
2. ç‚¹å‡»å³ä¸Šè§’"èŠå¤©æ¨¡å¼"æŒ‰é’®
3. è¾“å…¥æ¶ˆæ¯å¹¶å‘é€
4. è§‚çœ‹ AI å®æ—¶ç”Ÿæˆå›å¤

## ğŸ”§ é›†æˆçœŸå® AI

### æ–¹å¼ä¸€: ä½¿ç”¨ LiteLLM (å·²é›†æˆ)

ä¿®æ”¹ `web/backend/chat_api.py`:

```python
from pathlib import Path
from src.llm import LiteLLMClient

async def generate_stream_response(message: str):
    project_root = Path(__file__).parent.parent.parent
    config_path = project_root / "config" / "litellm_config.yaml"

    llm_client = LiteLLMClient(config_path=str(config_path))

    # LiteLLM æµå¼ç”Ÿæˆ
    full_response = await llm_client.generate(
        prompt=message,
        model="deepseek",
        max_tokens=2000,
        temperature=0.8
    )

    # é€å­—è¾“å‡º
    for char in full_response:
        data = {"type": "text", "content": char}
        yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

    yield f"data: {json.dumps({'type': 'done'})}\n\n"
```

### æ–¹å¼äºŒ: ä½¿ç”¨ Claude Agent SDK

```python
from claude_agent_sdk import query, ClaudeAgentOptions, AssistantMessage, TextBlock

async def generate_stream_response(message: str):
    options = ClaudeAgentOptions(
        allowed_tools=["Read", "Write", "Bash"],
        max_turns=1
    )

    async for msg in query(prompt=message, options=options):
        if isinstance(msg, AssistantMessage):
            for block in msg.content:
                if isinstance(block, TextBlock):
                    data = {"type": "text", "content": block.text}
                    yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

    yield f"data: {json.dumps({'type': 'done'})}\n\n"
```

### æ–¹å¼ä¸‰: ä½¿ç”¨ Vercel AI SDK (æ¨è)

åŸºäº Vercel AI Chatbot çš„æœ€ä½³å®è·µ:

```typescript
// å‰ç«¯ä½¿ç”¨ useChat hook
import { useChat } from "@ai-sdk/react"

const { messages, sendMessage, status } = useChat({
  api: "/api/chat/stream"
})
```

```python
# åç«¯ä½¿ç”¨ streamText
from ai import streamText

async def generate_stream_response(message: str):
    result = await streamText({
        "model": "deepseek",
        "messages": [{"role": "user", "content": message}]
    })

    # æµå¼è¾“å‡º
    async for chunk in result.textStream:
        data = {"type": "text-delta", "textDelta": chunk}
        yield f"data: {json.dumps(data)}\n\n"

    yield f"data: [DONE]\n\n"
```

## ğŸ“ ç›¸å…³æ–‡ä»¶

```
web/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI ä¸»åº”ç”¨(å·²æ³¨å†Œè·¯ç”±)
â”‚   â””â”€â”€ chat_api.py          # èŠå¤© API(æµå¼ç«¯ç‚¹)
â””â”€â”€ frontend/
    â””â”€â”€ app/
        â”œâ”€â”€ page.tsx         # ä¸»é¡µ(æœ‰"èŠå¤©æ¨¡å¼"æŒ‰é’®)
        â””â”€â”€ chat/
            â””â”€â”€ page.tsx     # èŠå¤©é¡µé¢(å·²ä¿®å¤)
```

## ğŸ› å·²ä¿®å¤é—®é¢˜

1. **æ¶ˆæ¯ç´¢å¼•é”™è¯¯**: ä¿®æ”¹ä¸ºä½¿ç”¨ `messages.length - 1`
2. **å†…å®¹ç´¯åŠ **: ä½¿ç”¨å±€éƒ¨å˜é‡ `assistantContent` ç´¯åŠ 
3. **CORS é…ç½®**: åç«¯å·²é…ç½®å…è®¸ localhost:3000

## ğŸ“ æ•°æ®æµ

```
ç”¨æˆ·è¾“å…¥
  â†“
å‰ç«¯å‘é€ POST /api/chat/stream
  â†“
åç«¯ç”Ÿæˆæµå¼å“åº” (SSE)
  â†“
data: {"type": "text", "content": "ä½ "}
data: {"type": "text", "content": "å¥½"}
...
data: {"type": "done"}
  â†“
å‰ç«¯å®æ—¶æ›´æ–° UI
  â†“
æ˜¾ç¤ºå®Œæ•´æ¶ˆæ¯
```

## ğŸ¯ ä¸‹ä¸€æ­¥

1. âœ… åŸºç¡€èŠå¤©ç•Œé¢
2. âœ… æµå¼è¾“å‡ºæ˜¾ç¤º
3. â³ è¿æ¥çœŸå® AI (3ç§æ–¹å¼å¯é€‰)
4. â³ å¯¹è¯å†å²æŒä¹…åŒ–
5. â³ å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡

## ğŸ’¡ æç¤º

- å½“å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®,éƒ¨ç½²æ—¶æ›¿æ¢ `chat_api.py` ä¸­çš„ `generate_stream_response` å‡½æ•°
- æ¨èä½¿ç”¨å·²é›†æˆçš„ LiteLLM + DeepSeek V3 æ¨¡å‹
- å‚è€ƒ Vercel AI Chatbot çš„ `useChat` hook å¯ä»¥ç®€åŒ–å‰ç«¯ä»£ç 
